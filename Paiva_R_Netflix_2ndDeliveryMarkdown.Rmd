---
title: "Paiva_R_Netflix_2ndDeliveryMarkdown"
author: "Isadora Campregher Paiva"
date: "2022-12-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Netflix Data Analysis Project - Advanced

This is a project made in the context of the Tech Academy R for Data Science Advanced course at the Goethe University Frankfurt in 2022/2023. We had to use a general Netflix dataset from Kaggle in addition to our own data and go through a series of analyses, as you'll see below.

# 4.1 Getting started
### Import libraries

```{r, echo=TRUE, warning=FALSE,message=FALSE}
library(tidyverse)
library(lubridate)
library(plotly)
library(ggExtra)
library(wordcloud)
```

### Load dataset 
I then set my working directory and load the dataset [Netflix Movies and TV Shows](https://www.kaggle.com/datasets/shivamb/netflix-shows). I'm working with Version 5, last updated on September 27th, 2021.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
# Set directory
setwd("D:/R_TechAcademy")

#Load general Netflix data
netflix_general <- read_csv("netflix_titles.csv")
```

## 4.1.1 Discovering the Data
First, we need to get an overview of the data.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
glimpse(netflix_general)
summary(netflix_general)
```

All columns are set as having characters as a class, except for release_year, which is a double (numeric). The TechAcademy Leitfaden asks us to turn the release_year into a date, though I found that to be a waste of time, because even with the lubridate library converting to year, we still eventually go back to a double since the format of a year (dddd) is not actually a date format. Here is the code that I used (do correct me if I'm wrong!):

```{r, echo=TRUE, warning=FALSE,message=FALSE}
# Convert relese_year to date format
netflix_general$release_year <- year(as.Date(as.character(netflix_general$release_year), format = "%Y"))
# Check to see if that worked
class(netflix_general$release_year)
```
We should make "date_added" a date. Here it works well.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
# Convert date_added to date format
netflix_general$date_added <- mdy(netflix_general$date_added)
# Check to see if that worked
class(netflix_general$date_added)
```
We should also make "duration" numeric. This is a bit more complicated, because the original dataset mixes duration in minutes for films and duration in seasons for TV Shows. We should separate those two into different columns, "duration_movie" and "duration_season_number". The cells for the wrong category should be left at "NA". 

```{r, echo=TRUE, warning=FALSE,message=FALSE}
netflix_general <- netflix_general %>%
  mutate(duration_movie = as.numeric(gsub('min', '', ifelse(type=='Movie', duration, NA))),
         duration_season_number = as.numeric(gsub('[ Season Seasons]', '', ifelse(type=='TV Show', duration, NA))))
```

Let's remove the "duration" column to avoid confusion later on

```{r, echo=TRUE, warning=FALSE,message=FALSE}
netflix_general <- select(netflix_general, -"duration")
```

## 4.1.2 Give some overall statements

### Longest movie

What’s the longest movie (not TV show) included in the dataset?

```{r, echo=TRUE, warning=FALSE,message=FALSE}
print(netflix_general %>% slice_max(duration_movie))
```
The film is called *Black Mirror: Bandersnatch* at 312 minutes. This is actually an interesting case, since this film is a "choose your own adventure" special episode of the the anthology series *Black Mirror*. Since it is interactive, there is actually no set amount of time the film takes, though [Netflix themselves](https://www.bustle.com/p/how-long-is-bandersnatch-you-can-literally-get-lost-in-this-black-mirror-movie-15574442) affirm that the run time for default choices is only 90 minutes. This shows how individual cases can often defy simple characterizations like "duration". The 312 minutes that we see here are likely the cumulative duration of all available scenes, though a single screening of the film could never be that long.

### Most represented country

Which country has the most content (movies and tv shows) featured on Netflix?

```{r, echo=TRUE, warning=FALSE,message=FALSE}
print(netflix_general %>% 
  count(country) %>%
  slice_max(n))
```
The answer is not surprising - the United States, with 2818 media types in the dataset.

### Overall numbers: movies and TV 

How many movies and tv shows are included? Let's make a barchart:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
ggplot(netflix_general, aes(x = type)) + geom_bar() + 
  labs(x=NULL, y=NULL, title="Number of Media on Netflix by Type")
```


# 4.2 Data Cleaning and Useful Transformations

## 4.2.1 Date Formatting

I had already done the suggested cleanups (conversion of “date_added” and “release_year” into date format) above in section 4.1.

## 4.2.2 More details of the longest movie

What is the mean and standard deviation of movie duration in minutes?

```{r, echo=TRUE, warning=FALSE,message=FALSE}
# mean
print(duration_mean <- mean(netflix_general$duration_movie, na.rm=T))
# standard deviation
print(duration_sd <- sd(netflix_general$duration_movie, na.rm=T))
```

## 4.2.3 A ~~histogram~~ bar chart of the top 10 longest movies duration

First, let's take a look at what are the 10 longest films on Netflix

```{r, echo=TRUE, warning=FALSE,message=FALSE}
top_ten_length <- netflix_general %>%
  slice_max(duration_movie, n = 10)
print(top_ten_length)
```

Now, let's turn this into a bar chart (while the Leitfaden calls this a histogram, it's actually a bar chart since the data is discrete - each film is a separate entity with its own length - hence the gaps between each bar, which do not exist in a histogram).

```{r, echo=TRUE, warning=FALSE,message=FALSE}
ggplot(top_ten_length, aes(x = duration_movie, y = reorder(title, duration_movie))) + 
  geom_col() +
  labs(x="Duration (min)", y=NULL, title="The 10 Longest Movies on Netflix")
```

## 4.2.4 Visualizing average movie durations over time

The Leitfaden suggests at this point that we analyze how the average movie length evolved with a graph.


```{r, echo=TRUE, warning=FALSE,message=FALSE}
ggplot(netflix_general, aes(x = release_year, y = duration_movie)) + 
  geom_line(stat = "summary", fun = "mean")
```

We were asked to comment and interpret the graph - "were there any significant increases/decreases in movie length over time? If so, what could be the reason?" 

I actually don't believe this graph of averages can tell us much about this, because the means hide the number of films they are based on per year. So I made a scatter plot with the film's title as a hover text to better inspect the data. (You can count this as my "surprise us" plot for 4.8)

```{r, echo=TRUE, warning=FALSE,message=FALSE}
scatter_length <- ggplot(netflix_general, aes(x = release_year, y = duration_movie, text=title)) +  
  geom_point() +
  labs(x="Release Year", y="Duration (min)", title="Netflix's movie durations over time")
ggplotly(scatter_length)
```

After actually checking the films in question, I believe this is just a quirk of the data, and cannot tell us anything about actual trends in the length of films *produced* throughout the decades. The Netflix dataset has few movies from the 1940s to the 1980s, making the length of individual films in this period skew the results. There is a cluster of World War 2 documentaries that have around 40 minutes that are bringing the average down in the 1940s, while in the 1960s a few epics like *Doctor Zhivago* are skewing durations up, but that is not representative of the average duration of films made in these periods. There are also quite a few "movies" bringing the average down in the last few decades that aren't actually feature films, like "Power Rangers specials", comedy specials, and special features (e.g. *Creating the Queen's Gambit*).

# 4.3 Your personal data

In this portion, each participant used a data set based on their own viewing activity, which they requested from Netflix.

## 4.3.1 Load your data

```{r, echo=TRUE, warning=FALSE,message=FALSE}
my_data <- read_csv("D:/R_TechAcademy/NetflixReport/CONTENT_INTERACTION/ViewingActivity.csv")
```
Let's see what it looks like:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
glimpse(my_data)
```
The data types are better than in netflix_general, but there are still some issues we need to solve. 

## 4.3.2 Clean and transform dataset

The cleaning here involves several steps. First, we remove things that aren't films by taking out anything that isn't NA in "Supplemental Video Type". This category is just for trailers, recaps, special features, etc. Then, we run into the issue that the column "Title" specifies TV show episodes by name, making it too granular and incompatible with the larger Netflix dataset we have, where series titles are isolated. 

Since we will want to merge those two, they need to fit together. We need to split that column into 3 columns - 'title' (lower-case, as in 'netflix_general'), 'season' and 'episode_title'. This information is split on colons (:), but we can't just split them like that here, because some films have colons on their names. So we first check that the column 'title' has words like "season" and equivalents to send that information to the corresponding column, and we do the same with the word "episode" and equivalent, pushing it to the column 'episode_title'.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
my_data <- my_data %>%
  #filter out supplemental videos
  filter(is.na(`Supplemental Video Type`)) %>% 
  #separate titles for TV show episodes
  separate(col=Title, into=c('title','season', 'episode_title'), 
           sep=': ', remove=TRUE) %>% 
  mutate(title=ifelse(grepl('Season', season) | 
                        grepl('Series', season) | 
                        grepl('Staffel', season) |
                        grepl('Episode', episode_title) | 
                        grepl('Chapter', episode_title) |
                        is.na(season),  title, paste(title, season, sep=': ')),
         # Identify how "start Time" is organized
         `Start Time`= ymd_hms(`Start Time`),
         # Split into columns for date, month, weekday and start time, which we'll need later
         viewing_date = date(`Start Time`),
         viewing_month = month(`Start Time`, label=TRUE),
         viewing_year = year(`Start Time`),
         viewing_weekday = wday(`Start Time`, label=TRUE),
         start_time = hms(format(as.POSIXct(`Start Time`), format = "%H:%M:%S")))%>%
  # Rename "Duration" column to watch_time to avoid confusion
  rename(watch_time = Duration)
```

The TechAcademy Leitfaden says here that "Netflix recorded every time you clicked on a movie even if you didn’t watch it. Check which column indicates those with a specific value." I imagine you are referring to the column "Attributes", which marks if a film was autoplayed, but I don't think this matters much, as an autoplayed film might still have been watched. I would agree that things with a very short watch_time might be better off removed to avoid bias, but looking at the data, it seems this often happens because of pauses, so that the cumulative watch_time should include those short bursts, so I'll keep those in for now, and filter them out when needed.

Before we join the datasets, let's remove the columns we don't want from my_data so the joined data isn't unnecessarily large.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
my_data <- select(my_data, -c('Start Time', 'Attributes', 'Supplemental Video Type', 'Device Type', 'Bookmark', 'Latest Bookmark', 'Country'))
```

The only column now that still had spaces in the name was "Profile Name". Let's change that because it's kind of annoying, sometimes causing problems with selection.

```{r}
my_data <- my_data %>% 
  rename(profile_name = 'Profile Name')
```


## 4.3.3 Join datasets

Now let's join! Both datasets have a column called "title" which is reasonably clean now, so let's use that.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
netflix_combined <- my_data %>% 
  left_join(netflix_general, by= "title")
```

## 4.3.5 ~~Dynamic~~ Interactive line plot
Our goal for this task is to plot how each viewer’s activity was recorded over time. First, we need to group the watch times per day for each viewer (this if you have different viewers in your account, as I do):

```{r, echo=TRUE, warning=FALSE,message=FALSE}
by_date <- netflix_combined %>% 
  group_by(profile_name, viewing_date) %>% 
  mutate(watchtime_per_day = as.period((sum(watch_time))))
```
  
TechAcademy recommended a dynamic chart here since it would be a bit unclear in a static plot, but I figured it would be even clearer as an interactive plot:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
per_day_plot <- ggplot(by_date, aes(y = watchtime_per_day, x=viewing_date, color=profile_name))+
  geom_point()+
  geom_line()+
  scale_y_time(name= "Watch time per day")+
  scale_x_date(date_breaks="9 month", date_labels = "%Y-%m")+ 
  theme(axis.text.x = element_text(angle= 45, hjust=1))
# Make it interactive  
ggplotly(per_day_plot)
```
My mother, Glaucia, seems to be the big binge watcher in the family! She has some extraordinary bursts (17h and 34 minutes on May 30th, 2021!) of activity that are almost suspiciously long. I'll investigate that in a bit. The other users binge-watching activity is more normal, with peaks around 8h in a day. Glaucia, Caio and Isa (that's me) all started using the Netflix account in 2017, but Samuel only started in April 2020 - perhaps a pandemic-related change in viewing habits?

```{r}
netflix_combined %>% 
  filter(profile_name == "glaucia") %>% 
  filter(viewing_date == "2021-05-30")
```
Ok, there's definitely something weird going on, I doubt my mom is going on hour-long binges of iCarly! I asked her about this and she can't think of anything other than her account info is being used by someone else that she definitely did not approve of! This suspicious activity continued for months, though it seems to have died down in mid-2022, though we should change our password just the same.

# 4.4 Let's get personal

Next, we'll investigate my own viewing habits. What's the longest movie I have ever watched on Netflix? 

```{r, echo=TRUE, warning=FALSE,message=FALSE}
isa_longest <- netflix_combined %>%
  filter(profile_name == "Isa") %>%
  slice_max(duration_movie)
print(isa_longest)
```
I tried getting through *The irishman* on three different days and in the end I never finished the movie, it was too long...

## 4.4.1 Monthly viewing time in 2021

Now, we are supposed to analyze how my viewing time has changed throughout one year, 2021. First, let's filter for my profile and the year 2021, then group by month and add the watch time up:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
isa_2021_month_watchtime <- netflix_combined %>%
  filter(profile_name =="Isa" & viewing_year == 2021) %>%
  group_by(viewing_month) %>% 
  mutate(watchtime_per_month = sum(watch_time))
```

Now let's select just the columns we need - the month and watch time per month - and remove duplicates:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
isa_2021_month_watchtime <- select(isa_2021_month_watchtime, c('viewing_month', 'watchtime_per_month'))
isa_2021_month_watchtime <- unique(isa_2021_month_watchtime)
```

Now let's make a graph:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
ggplot(isa_2021_month_watchtime, aes(x=viewing_month, y=watchtime_per_month)) + 
  geom_col() +
  labs(x="Month", title="Isa's Monthly Watch Times 2021") +
  scale_y_time(name='Watch time (hh:mm:ss)')
```

There is more variation than I was expecting. The dip in June and August could be explained as the summer months - better weather, more time outside - but the surge in July goes against that logic. I started a new job in April and was teaching from April to June, so that might contribute to the diminished watch times then. 

## 4.4.2 Average per weekday

Our subsequent interest lies in analyzing the viewing time of specific weekdays. On which days have you watched more Netflix, can you see a peak? For our advanced programmers: Would it not be interesting to look at what time of the day you watch the most?

We'll use a similar formula as last time, but now we'll group by weekday and we'll do the average instead of the sum:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
isa_2021_weekday_watchtime <- netflix_combined %>%
  filter(profile_name =="Isa" & viewing_year == 2021) %>%
  group_by(viewing_weekday) %>% 
  mutate(watchtime_per_weekday = mean(watch_time))
```

Now let's select just the columns we need - the day and watch time per month - and remove duplicates:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
isa_2021_weekday_watchtime <- select(isa_2021_weekday_watchtime, c('viewing_weekday', 'watchtime_per_weekday'))
isa_2021_weekday_watchtime <- unique(isa_2021_weekday_watchtime)
```

Now let's make a graph:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
ggplot(isa_2021_weekday_watchtime, aes(x=viewing_weekday, y=watchtime_per_weekday)) + 
  geom_col() +
  scale_y_time(name='Watch time (hh:mm:ss)') +
  labs(x = "Weekday", title="Isa's Average Watch Times per Weekday 2021")
```

Saturday is second-to-last (after Monday) in my watch times, defying my preconceived notion that I'd watch more on the weekend - though Sunday is highest, as expected.

## 4.5 Binge watching
In this section, the goal should be to create a plot of my top 10 binge TV shows. We first want to filter for TV Shows and group by date and title. We then sum the watch time of each title per day.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
binge_TV <- netflix_combined %>%
  filter(profile_name =="Isa" & type == "TV Show") %>%
  group_by(title, viewing_date) %>% 
  mutate(watchtime_per_session = sum(watch_time))
```

Now let's select just the columns we need - title, day and watch time per session - and remove duplicates:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
binge_TV <- select(binge_TV, c('title', 'viewing_date', 'watchtime_per_session'))
binge_TV <- unique(binge_TV)
```

Let's take a look at the data

```{r, echo=TRUE, warning=FALSE,message=FALSE}
binge_TV[order(-binge_TV$watchtime_per_session),]
```
Sometimes the same TV Show appears multiple times, since we are counting the top binge *sessions* here, not TV Shows. There are several ways for us to define and analyze the "top 10 binge TV Shows". I'm going to filter out the watch sessions that were very short (following Netflix's own practices, a watch time of less than 2 minutes). I will then group the TV Shows by title and calculate the *median* watch time of that title.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
top_binge_TV <- binge_TV %>% 
    # Cut off value for movies not watched intentionally as defined by Netflix
  filter(watchtime_per_session > 120) %>%
  group_by(title) %>% 
  summarize(mean = mean(watchtime_per_session),
            sd = sd(watchtime_per_session),
            sum = sum(watchtime_per_session),
            median = median(watchtime_per_session))
```

I think in order to identify the shows that were the most "binge-worthy" for me, the median watch time per session makes the most sense, as it indicates the typical session and isn't swayed by outliers.

```{r, echo=TRUE, warning=FALSE,message=FALSE}
top10_binge_TV <- top_binge_TV %>%
  slice_max(median, n = 10)
print(top10_binge_TV)
```

And here as a graph:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
ggplot(top10_binge_TV, aes(x=reorder(title, median), y=median))+
  geom_col()+
  coord_flip()+
  scale_y_time(name='Median watch time (hh:mm:ss)') +
  labs(x="TV Show", title="Isa's Median Watch Times per TV Show Binge session")
```

I share my Netflix account with my boyfriend, and looking at this table makes me realize he is more of a binge watcher than me. The number one position by a wide margin - *Marvel's The Defenders* - is something he watched by himself, while the second and third place *Next in Fashion* and *Stranger Things*, we watched together.

## 4.6 Scatterplot with marginal density

How has the watching behavior of my family developed on Netflix since we first started using it? We will visualize this via a scatterplot with marginal density. Include all the profile names for this task and make a visual comparison.

Let's filter out the things watched for less than two minutes:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
full_records <- netflix_combined %>% 
  filter(watch_time > 120)
```

And now let's make a scatterplot:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
scatter_marginal <- ggplot(data = full_records, 
       aes(x = viewing_date, y = watch_time)) +
  geom_point(aes(col = profile_name)) + 
  theme(legend.position = "bottom")
```

This is the code to add the marginal density: 

```{r, echo=TRUE, warning=FALSE,message=FALSE}
print(ggMarginal(scatter_marginal, type="density",
                   groupFill = TRUE,
                   groupColour = TRUE))
```

This plot is a bit too convoluted for my tastes and it's kind of hard to interpret so many datapoints at once. Since Netflix records every time you click on something, sessions with fits and starts are recorded multiple times, creating a wall of points for some days. Most watch sessions are quite short - everything below an hour becomes a mass of points, and as we can see from the density plot on the right, they cluster in particular around the half-hour mark. From the density plot above, we confirm some of the information we already had, like the fact that Samuel only started watching using this account in 2020, but has been watching steadily since then.

## 4.7 Word cloud with your favorite genre

Let's create a genre dataframe for my (Isa) genres:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
isa_genres <- netflix_combined %>% 
  filter(profile_name =="Isa") %>% 
  select(listed_in) %>%
  separate_rows(listed_in, sep=", ") %>%
  group_by(listed_in) %>% 
  summarize(freq=n())
```

And here's the wordcloud:
```{r, echo=TRUE, warning=FALSE,message=FALSE}
#set seed so that wordcloud remains the same
set.seed(401)
#attempt with Wordcloud
wordcloud(words=isa_genres$listed_in, freq=isa_genres$freq, min.freq = 5, rot.per = 0.3,
                     max.words = 200, random.order = FALSE, colors = brewer.pal(6, "Dark2"))
```

## 4.8 Surprise Us!

I added an interactive scatterplot beforehand (see 4.2.4) that wasn't asked for, so hopefully that covers the "surprise us" aspect :) 

# 5 Content-Based Recommendation System

The machine learning aspect, in which we must create a content-base recommendation system using cosine similarity of plot, genre and actors is due on February 5th, 2023.